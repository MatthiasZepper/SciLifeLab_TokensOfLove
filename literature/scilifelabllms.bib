@Article{Juzek2024,
  author        = {Juzek, Tom S. and Ward, Zina B.},
  title         = {Why Does ChatGPT "Delve" So Much? Exploring the Sources of Lexical Overrepresentation in Large Language Models},
  year          = {2024},
  month         = dec,
  abstract      = {Scientific English is currently undergoing rapid change, with words like "delve," "intricate," and "underscore" appearing far more frequently than just a few years ago. It is widely assumed that scientists' use of large language models (LLMs) is responsible for such trends. We develop a formal, transferable method to characterize these linguistic changes. Application of our method yields 21 focal words whose increased occurrence in scientific abstracts is likely the result of LLM usage. We then pose "the puzzle of lexical overrepresentation": WHY are such words overused by LLMs? We fail to find evidence that lexical overrepresentation is caused by model architecture, algorithm choices, or training data. To assess whether reinforcement learning from human feedback (RLHF) contributes to the overuse of focal words, we undertake comparative model testing and conduct an exploratory online study. While the model testing is consistent with RLHF playing a role, our experimental results suggest that participants may be reacting differently to "delve" than to other focal words. With LLMs quickly becoming a driver of global language change, investigating these potential sources of lexical overrepresentation is important. We note that while insights into the workings of LLMs are within reach, a lack of transparency surrounding model development remains an obstacle to such research.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution Share Alike 4.0 International},
  doi           = {10.48550/ARXIV.2412.11385},
  eprint        = {2412.11385},
  file          = {:Juzek2024 - Why Does ChatGPT _Delve_ so Much_ Exploring the Sources of Lexical Overrepresentation in Large Language Models.pdf:PDF:http\://arxiv.org/pdf/2412.11385v1},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Misc{Belanger2025,
  author       = {Ashley Belanger},
  howpublished = {Ars Technica},
  month        = jan,
  title        = {Torrenting from a corporate laptop doesn’t feel right: Meta emails unsealed},
  year         = {2025},
  abstract     = {Newly unsealed emails allegedly provide the "most damning evidence" yet against Meta in a copyright case raised by book authors alleging that Meta illegally trained its AI models on pirated books.

Last month, Meta admitted to torrenting a controversial large dataset known as LibGen, which includes tens of millions of pirated books. But details around the torrenting were murky until yesterday, when Meta's unredacted emails were made public for the first time. The new evidence showed that Meta torrented "at least 81.7 terabytes of data across multiple shadow libraries through the site Anna’s Archive, including at least 35.7 terabytes of data from Z-Library and LibGen," the authors' court filing said. And "Meta also previously torrented 80.6 terabytes of data from LibGen."

"The magnitude of Meta’s unlawful torrenting scheme is astonishing," the authors' filing alleged, insisting that "vastly smaller acts of data piracy—just .008 percent of the amount of copyrighted works Meta pirated—have resulted in Judges referring the conduct to the US Attorneys’ office for criminal investigation."},
  url          = {https://arstechnica.com/tech-policy/2025/02/meta-torrented-over-81-7tb-of-pirated-books-to-train-ai-authors-say/},
}

@Article{Zhang2024,
  author    = {Zhang, Manshu and Wu, Liming and Yang, Tao and Zhu, Bing and Liu, Yangai},
  journal   = {Surfaces and Interfaces},
  title     = {RETRACTED: The three-dimensional porous mesh structure of Cu-based metal-organic-framework - Aramid cellulose separator enhances the electrochemical performance of lithium metal anode batteries},
  year      = {2024},
  issn      = {2468-0230},
  month     = mar,
  pages     = {104081},
  volume    = {46},
  doi       = {10.1016/j.surfin.2024.104081},
  publisher = {Elsevier BV},
}

@misc{howard2018universal,
    title={Universal Language Model Fine-tuning for Text Classification},
    author={Jeremy Howard and others},
    year={2018},
    eprint={1801.06146},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@Book{Zipf1936,
  author    = {Zipf, George Kingsley},
  publisher = {Routledge},
  title     = {The psycho-biology of language: An introduction to dynamic philology},
  year      = {1936},
}

@misc{vaswani2017attention,
    title={Attention Is All You Need},
    author={Ashish Vaswani and others},
    year={2017},
    eprint={1706.03762},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{sun2019finetune,
    title={How to Fine-Tune BERT for Text Classification?},
    author={Chi Sun and others},
    year={2019},
    eprint={1905.05583},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{Gray2017GPUKF,
  title={GPU Kernels for Block-Sparse Weights},
  author={Scott Gray and others},
  year={2017}
}

@misc{yang2019xlnet,
    title={XLNet: Generalized Autoregressive Pretraining for Language Understanding},
    author={Zhilin Yang and others},
    year={2019},
    eprint={1906.08237},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{shoeybi2019megatronlm,
    title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
    author={Mohammad Shoeybi and others},
    year={2019},
    eprint={1909.08053},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{hochreiter1997long,
  added-at = {2016-11-15T08:49:43.000+0100},
  author = {Hochreiter, Sepp and others},
  biburl = {https://www.bibsonomy.org/bibtex/2a4a80026d24955b267cae636aa8abe4a/dallmann},
  interhash = {0692b471c4b9ae65d00affebc09fb467},
  intrahash = {a4a80026d24955b267cae636aa8abe4a},
  journal = {Neural computation},
  keywords = {lstm rnn},
  number = 8,
  pages = {1735--1780},
  publisher = {MIT Press},
  timestamp = {2016-11-15T08:49:43.000+0100},
  title = {Long short-term memory},
  volume = 9,
  year = 1997
}

@misc{wang2018glue,
    title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
    author={Alex Wang and others},
    year={2018},
    eprint={1804.07461},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{peters2018deep,
    title={Deep contextualized word representations},
    author={Matthew E. Peters and others},
    year={2018},
    eprint={1802.05365},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{wordPiece,
    title	= {Japanese and Korean Voice Search},
    author	= {Mike Schuster and others},
    year	= {2012},
    booktitle	= {International Conference on Acoustics, Speech and Signal Processing},
    pages	= {5149--5152}
}

@incollection{mikolov2013,
    title = {Distributed Representations of Words and Phrases and their Compositionality},
    author = {Mikolov, Tomas and others},
    booktitle = {Advances in Neural Information Processing Systems 26},
    editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
    pages = {3111--3119},
    year = {2013},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@inproceedings{penningtonglove,
    title = "{G}love: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      others",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and others},
  year={2018}
}

@inproceedings{mikolov2018advances,
  title={Advances in Pre-Training Distributed Word Representations},
  author={Mikolov, Tomas and others},
  booktitle={Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018)},
  year={2018}
}

@Article{Blum2023,
  author    = {Blum, Moritz},
  journal   = {Journal of Cardiac Failure},
  title     = {ChatGPT Produces Fabricated References and Falsehoods When Used for Scientific Literature Search},
  year      = {2023},
  issn      = {1071-9164},
  month     = sep,
  number    = {9},
  pages     = {1332--1334},
  volume    = {29},
  comment   = {doi: 10.1016/j.cardfail.2023.06.015},
  doi       = {10.1016/j.cardfail.2023.06.015},
  publisher = {Elsevier},
  url       = {https://doi.org/10.1016/j.cardfail.2023.06.015},
}

@Misc{Marcus2023,
  author       = {Gary Marcus},
  howpublished = {Marcus on AI},
  month        = feb,
  title        = {What Google Should Really Be Worried About},
  year         = {2023},
  url          = {https://garymarcus.substack.com/p/what-google-should-really-be-worried},
}

@Misc{Holmer2025,
  author       = {Freya Holmér},
  howpublished = {YouTube},
  month        = jan,
  title        = {Generative AI is a Parasitic Cancer},
  year         = {2025},
  url          = {https://www.youtube.com/watch?v=-opBifFfsMY},
}

@Article{Kobak2024,
  author        = {Kobak, Dmitry and González-Márquez, Rita and Horvát, Emőke-Ágnes and Lause, Jan},
  title         = {Delving into ChatGPT usage in academic writing through excess vocabulary},
  year          = {2024},
  month         = jun,
  abstract      = {Recent large language models (LLMs) can generate and revise text with human-level performance, and have been widely commercialized in systems like ChatGPT. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists have been using them to assist their scholarly writing. How wide-spread is LLM usage in the academic literature currently? To answer this question, we use an unbiased, large-scale approach, free from any assumptions on academic LLM usage. We study vocabulary changes in 14 million PubMed abstracts from 2010-2024, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. Our analysis based on excess words usage suggests that at least 10\% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, and was as high as 30\% for some PubMed sub-corpora. We show that the appearance of LLM-based writing assistants has had an unprecedented impact in the scientific literature, surpassing the effect of major world events such as the Covid pandemic.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution Share Alike 4.0 International},
  doi           = {10.48550/ARXIV.2406.07016},
  eprint        = {2406.07016},
  file          = {:http\://arxiv.org/pdf/2406.07016v2:PDF},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Computers and Society (cs.CY), Digital Libraries (cs.DL), Social and Information Networks (cs.SI), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@article{Radford2019,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@Article{Solaiman2019,
  author        = {Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeff and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and McCain, Miles and Newhouse, Alex and Blazakis, Jason and McGuffie, Kris and Wang, Jasmine},
  title         = {Release Strategies and the Social Impacts of Language Models},
  year          = {2019},
  month         = aug,
  abstract      = {Large language models have a range of beneficial uses: they can assist in prose, poetry, and programming; analyze dataset biases; and more. However, their flexibility and generative capabilities also raise misuse concerns. This report discusses OpenAI's work related to the release of its GPT-2 language model. It discusses staged release, which allows time between model releases to conduct risk and benefit analyses as model sizes increased. It also discusses ongoing partnership-based research and provides recommendations for better coordination and responsible publication in AI.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1908.09203},
  eprint        = {1908.09203},
  file          = {:Solaiman2019 - Release Strategies and the Social Impacts of Language Models.pdf:PDF:http\://arxiv.org/pdf/1908.09203v2},
  keywords      = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Computers and Society (cs.CY), FOS: Computer and information sciences, I.2; I.2.7; K.4},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Misc{Penedo2024,
  author       = {Guilherme, Penedo and Hynek, Kydlíček and Loubna, Ben Allal and Anton, Lozhkov and Colin Raffel and Leandro, Werra and Thomas, Wolf},
  howpublished = {Hugging Face Blog},
  month        = may,
  title        = {FineWeb: decanting the web for the finest text data at scale},
  year         = {2024},
  url          = {https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1},
}

@Article{Bahdanau2014,
  author        = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  title         = {Neural Machine Translation by Jointly Learning to Align and Translate},
  year          = {2014},
  month         = sep,
  abstract      = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1409.0473},
  eprint        = {1409.0473},
  file          = {:Bahdanau2014 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf:PDF:http\://arxiv.org/pdf/1409.0473v7},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Misc{Alammar2018,
  author       = {Jay Alammar},
  howpublished = {Blog post},
  month        = jun,
  title        = {The Illustrated Transformer},
  year         = {2018},
  url          = {http://jalammar.github.io/illustrated-transformer/},
}

@Article{Borgeaud2021,
  author        = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Driessche, George van den and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack W. and Elsen, Erich and Sifre, Laurent},
  title         = {Improving language models by retrieving from trillions of tokens},
  year          = {2021},
  month         = dec,
  abstract      = {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2112.04426},
  eprint        = {2112.04426},
  file          = {:Borgeaud2021 - Improving Language Models by Retrieving from Trillions of Tokens.pdf:PDF:http\://arxiv.org/pdf/2112.04426v3},
  keywords      = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Misc{Alammar2022,
  author       = {Jay Alammar},
  howpublished = {Blog post},
  month        = jan,
  title        = {The Illustrated Retrieval Transformer},
  year         = {2022},
  abstract     = {The latest batch of language models can be much smaller yet achieve GPT-3 like performance by being able to query a database or search the web for information. A key indication is that building larger and larger models is not the only way to improve performance.},
  url          = {https://jalammar.github.io/illustrated-retrieval-transformer/},
}

@Misc{Nayak2019,
  author   = {Pandu Nayak},
  month    = oct,
  title    = {Understanding searches better than ever before},
  year     = {2019},
  abstract = {Applying BERT models to Search
Last year, we introduced and open-sourced a neural network-based technique for natural language processing (NLP) pre-training called Bidirectional Encoder Representations from Transformers, or as we call it--BERT, for short. This technology enables anyone to train their own state-of-the-art question answering system.},
  url      = {https://blog.google/products/search/search-language-understanding-bert/},
}

@Misc{Zhu2019,
  author       = {Jeffrey Zhu},
  howpublished = {Microsoft Azure Blog},
  month        = nov,
  title        = {Bing delivers its largest improvement in search experience using Azure GPUs},
  year         = {2019},
  abstract     = {Recently, there was a breakthrough in natural language understanding with a type of model called transformers (as popularized by Bidirectional Encoder Representations from Transformers, BERT). Unlike previous deep neural network (DNN) architectures that processed words individually in order, transformers understand the context and relationship between each word and all the words around it in a sentence. Starting from April of this year, we used large transformer models to deliver the largest quality improvements to our Bing customers in the past year. For example, in the query “what can aggravate a concussion”, the word “aggravate” indicates the user wants to learn about actions to be taken after a concussion and not about causes or symptoms. Our search powered by these models can now understand the user intent and deliver a more useful result. More importantly, these models are now applied to every Bing search query globally making Bing results more relevant and intelligent.},
  url          = {https://azure.microsoft.com/en-us/blog/bing-delivers-its-largest-improvement-in-search-experience-using-azure-gpus/},
}

@Article{Piantadosi2014,
  author          = {Piantadosi, Steven T.},
  journal         = {Psychonomic bulletin \& review},
  title           = {Zipf's word frequency law in natural language: a critical review and future directions.},
  year            = {2014},
  issn            = {1531-5320},
  month           = oct,
  pages           = {1112--1130},
  volume          = {21},
  abstract        = {The frequency distribution of words has been a key object of study in statistical linguistics for the past 70 years. This distribution approximately follows a simple mathematical form known as Zipf's law. This article first shows that human language has a highly complex, reliable structure in the frequency distribution over and above this classic law, although prior data visualization methods have obscured this fact. A number of empirical phenomena related to word frequencies are then reviewed. These facts are chosen to be informative about the mechanisms giving rise to Zipf's law and are then used to evaluate many of the theoretical explanations of Zipf's law in language. No prior account straightforwardly explains all the basic facts or is supported with independent evaluation of its underlying assumptions. To make progress at understanding why language obeys Zipf's law, studies must seek evidence beyond the law itself, testing assumptions and evaluating novel predictions with new, independent data.},
  citation-subset = {IM},
  completed       = {2015-06-22},
  country         = {United States},
  doi             = {10.3758/s13423-014-0585-6},
  issn-linking    = {1069-9384},
  issue           = {5},
  keywords        = {Humans; Language; Linguistics, methods; Models, Theoretical; Semantics; Statistics as Topic; Vocabulary},
  mid             = {NIHMS579165},
  nlm-id          = {9502924},
  owner           = {NLM},
  pmc             = {PMC4176592},
  pmid            = {24664880},
  pubmodel        = {Print},
  pubstate        = {ppublish},
  revised         = {2022-03-11},
}

@Article{Li1992,
  author   = {Li, Wentian},
  journal  = {IEEE Transactions on Information Theory},
  title    = {Random texts exhibit Zipf's-law-like word frequency distribution},
  year     = {1992},
  number   = {6},
  pages    = {1842-1845},
  volume   = {38},
  doi      = {10.1109/18.165464},
  keywords = {Frequency;Natural languages;Statistics;Testing;Books},
}

@Article{Devlin2018,
  author        = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  title         = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  year          = {2018},
  month         = oct,
  abstract      = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arXiv},
  copyright     = {arXiv.org perpetual, non-exclusive license},
  doi           = {10.48550/ARXIV.1810.04805},
  eprint        = {1810.04805},
  file          = {:Devlin2018 - BERT_ Pre Training of Deep Bidirectional Transformers for Language Understanding.pdf:PDF:http\://arxiv.org/pdf/1810.04805v2},
  keywords      = {Computation and Language (cs.CL), FOS: Computer and information sciences},
  primaryclass  = {cs.CL},
  publisher     = {arXiv},
}

@Article{Zhou2023,
  author        = {Zhou, Zhihan and Ji, Yanrong and Li, Weijian and Dutta, Pratik and Davuluri, Ramana and Liu, Han},
  title         = {DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome},
  year          = {2023},
  month         = jun,
  abstract      = {Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenization. Based on these insights, we introduce DNABERT-2, a refined genome foundation model that adapts an efficient tokenizer and employs multiple strategies to overcome input length constraints, reduce time and memory expenditure, and enhance model capability. Furthermore, we identify the absence of a comprehensive and standardized benchmark for genome understanding as another significant impediment to fair comparative analysis. In response, we propose the Genome Understanding Evaluation (GUE), a comprehensive multi-species genome classification dataset that amalgamates $36$ distinct datasets across $9$ tasks, with input lengths ranging from $70$ to $10000$. Through comprehensive experiments on the GUE benchmark, we demonstrate that DNABERT-2 achieves comparable performance to the state-of-the-art model with $21 \times$ fewer parameters and approximately $92 \times$ less GPU time in pre-training.},
  archiveprefix = {arXiv},
  copyright     = {Creative Commons Attribution 4.0 International},
  doi           = {10.48550/ARXIV.2306.15006},
  eprint        = {2306.15006},
  file          = {:Zhou2023 - DNABERT 2_ Efficient Foundation Model and Benchmark for Multi Species Genome.pdf:PDF:http\://arxiv.org/pdf/2306.15006v2},
  keywords      = {Genomics (q-bio.GN), Artificial Intelligence (cs.AI), Computational Engineering / Finance / Science (cs.CE), Computation and Language (cs.CL), FOS: Biological sciences, FOS: Computer and information sciences},
  primaryclass  = {q-bio.GN},
  publisher     = {arXiv},
}

@Article{Ji2021,
  author          = {Ji, Yanrong and Zhou, Zhihan and Liu, Han and Davuluri, Ramana V.},
  journal         = {Bioinformatics (Oxford, England)},
  title           = {DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome.},
  year            = {2021},
  issn            = {1367-4811},
  month           = aug,
  pages           = {2112--2120},
  volume          = {37},
  abstract        = {Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios. To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks. The source code, pretrained and finetuned model for DNABERT are available at GitHub (https://github.com/jerryji1993/DNABERT). Supplementary data are available at Bioinformatics online.},
  citation-subset = {IM},
  country         = {England},
  doi             = {10.1093/bioinformatics/btab083},
  issn-linking    = {1367-4803},
  issue           = {15},
  nlm-id          = {9808944},
  owner           = {NLM},
  pii             = {6128680},
  pmc             = {PMC11025658},
  pmid            = {33538820},
  pubmodel        = {Print},
  pubstate        = {ppublish},
  revised         = {2024-04-25},
}

@Comment{jabref-meta: databaseType:bibtex;}
