% !TeX document-id = {55cdd1f2-c708-43a6-9dcf-ca7bc9e91f17}
% !BIB program = biber
\documentclass[10pt]{beamer}

\usetheme{scilifelab}
\usepackage{appendixnumberbeamer}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=scAqua, urlcolor=scAqua, citecolor=scAqua}

\usepackage[backend=biber,style=apa, sortcites=true,sorting=nyt]{biblatex}
\addbibresource{literature/scilifelabllms.bib}


\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{scilifelab}}\xspace}
\newcommand{\credit}[1]{{\par \raggedleft \scriptsize \mdseries \color{mDarkBrown} #1 \par}}
\newcommand{\creditdark}[1]{{\par \raggedleft \scriptsize \mdseries \color{scMGray} #1 \par}}
\newcommand{\creditleft}[1]{{\par \raggedright \scriptsize \mdseries \color{mDarkBrown} #1 \par}}
\newcommand{\creditdarkleft}[1]{{\par \raggedright \scriptsize \mdseries \color{scMGray} #1 \par}}
\newcommand{\citeme}[1]{{\xspace\color{scAqua} \scriptsize [\cite{#1}]}}
\newcommand{\feature}[1]{{\color{scLime} \textbf{#1}}}
\newcommand{\remark}[1]{{\par \color{scGrape} \ensuremath{\rightarrow} \emph{#1}}}

\makeatletter
\newcommand*{\myroman}[1]{{\fontfamily{ptm}\selectfont \expandafter\@slowromancap\romannumeral #1@}}
\makeatother

\title{Token(s) of love}
\subtitle{Potentials and pitfalls of using Large Language Models}
\date{February 14, 2025}
\author{Matthias Zepper, PhD}
\institute{NGI Stockholm, Genomic Focus Meeting \par \href{https://github.com/MatthiasZepper/SciLifeLab_TokensOfLove}{https://github.com/MatthiasZepper/SciLifeLab\_TokensOfLove}}
\titlegraphic{\hfill\includegraphics[height=1cm]{./additional_graphics/SciLifeLab_Logotype_Green_POS.png}}

\begin{document}

\maketitle

\begin{frame}{Valentine's day 2025: Mankind in love with generative AI models}
\begin{figure}
	\includegraphics[width=0.8\textwidth]{figures/Valentine_s_Day_oil_painting_mathematic_matrices_formulas_and_computers.png}
	\caption{A person in love with AI models and computers. Created with the open-weights 'FLUX.1 [schnell]' model by Black Forest Labs.}
\end{figure}
\credit{https://blackforestlabs.ai, https://github.com/black-forest-labs/flux}
%%invisible fake citation needed for Biber to pick up any citations, since I am using a custom cite command.
\vspace{2cm}
\cite{Blum2023}
\end{frame}

\begin{frame}{AI integrations and services everywhere}
	\begin{columns}[T,onlytextwidth]
		\hspace*{-1.1cm} 
		\column{0.2\textwidth}
		\begin{figure}
			\includegraphics[width=\textwidth]{figures/Valentine_s_Day_gift_card_square.png}
		\end{figure}
		\column{0.8\textwidth}
		\begin{itemize}
			\item One-click automated data analysis and LLM-guided interpretation
			\item AI pipeline developer \& QC report interpreter
			\item Synthetic DNA generation for strains or antibody optimization. 
		\end{itemize}
	\vspace{0.3cm} 
	\end{columns}
	\begin{columns}[T,onlytextwidth]
		\hspace*{-1.1cm} 
		\column{0.4\textwidth}
		\begin{figure}
			\includegraphics[width=\textwidth]{figures/GenerativeAI_DNA_Analysis_Mithrl.png}
			\creditdark{https://www.mithrl.com}
		\end{figure}
		\column{0.4\textwidth}
		\begin{figure}
			\includegraphics[width=\textwidth]{figures/GenerativeAI_DNA_Analysis_Seqera.png}
			\creditdark{https://seqera.io/ask-ai/}
		\end{figure}
			\column{0.4\textwidth}
		\begin{figure}
			\includegraphics[width=\textwidth]{figures/GenerativeAI_DNA_Analysis_Ginkgo.png}
			\creditdark{https://www.ginkgo.bio/platform \xspace}
		\end{figure}
	\end{columns}
\end{frame}

\begin{frame}{Encounters with AI generated content are inevitable...*}
	
	{\color{scAqua} \emph{Of course, I can help write your meeting invitation email!}}
	\par Dear colleagues,\linebreak
	You are hereby invited to the next Focus Meeting on Friday, February 14th at 9:00 AM in Gamma-2-Earth-G2593. {\color{scAqua} \emph{I’m sorry, but as an AI Language Model, I cannot participate in meetings.}}
	\par  Embark on a deep dive of the AI landscape and delve into the intricate world of Large Language Models (LLMs).
	Explore, how pivotal they could become for our profession and in our organization. {\color{scAqua} \emph{Based on the information provided}} we’ll begin with the fundamentals of LLMs, cover some major applications and lastly address their suitability for real-world use cases at the NGI.
	\par Looking forward to a lively exchange of ideas.
	\par Best,\linebreak
	{\color{scAqua} \emph{$\left[Your Name\right]$}}
	
	\credit{*actually not AI-generated, because ChatGPT is a poor impersonator of its earlier versions}
\end{frame}

\begin{frame}{The intricate tapestry of scientific language has been disrupted}
	\begin{columns}[T,onlytextwidth]
		\hspace*{-0.7cm} 
		\column{0.5\textwidth}
		\begin{figure}
			\includegraphics[width=\textwidth]{figures/ZhangTitle.png}
			\includegraphics[width=\textwidth]{figures/ZhangAbstractCrop.png}
			\caption{Now retracted article with text duplication and Generative AI use without disclosure.\citeme{Zhang2024}}
		\end{figure}

		\column{0.5\textwidth}
		
		\begin{figure}
			\includegraphics[width=1.17\textwidth]{figures/Juzek2024-Fig4.png}
			\caption{Words like “\href{https://pubmed.ncbi.nlm.nih.gov/?term=\%22delving\%20into\%22\&filter=years.2010-2024&timeline=expanded}{delving into}”, “\href{https://pubmed.ncbi.nlm.nih.gov/?term=\%22intricate\%22&filter=years.2010-2024&timeline=expanded}{intricate}” and “\href{https://pubmed.ncbi.nlm.nih.gov/?term=\%22tapestry\%22&filter=years.2010-2024&timeline=expanded}{tapestry}” recently appear far more frequently in abstracts of scientific publications, likely reflecting LLM use for copy editing.\citeme{Kobak2024,Juzek2024}}
		\end{figure}

	\end{columns}
\end{frame}

\begin{frame}{Worse without peer-review, editorial process or educated audience}
	\begin{figure}
		\hspace*{-1cm} 
		\includegraphics[height=0.97\textheight]{figures/Cliffs.png}
	\end{figure}
\end{frame}

\begin{frame}{Search engines start failing to find meaningful content}
	\begin{columns}[T,onlytextwidth]
		\hspace*{-0.7cm} 
		\column{0.5\textwidth}
		\begin{figure}
			\includegraphics[width=\textwidth]{figures/marcusg-google.png}
			\caption{Early essay extending the model collapse scenario to search engines and the world wide web.\citeme{Marcus2023}}
		\end{figure}
		\column{0.5\textwidth}
		
		\begin{figure}
			\includegraphics[width=1.17\textwidth]{figures/holmer-parasiticcancer.png}
			\caption{For some technical terms, all major search engines overweight AI-generated content heavily.\citeme{Holmer2025}}
		\end{figure}
		
	\end{columns}
\end{frame}

\begin{frame}[standout]{Whatever the future will bring...}
	\begin{columns}[T,onlytextwidth]
		\normalfont \normalsize
		\hspace*{-0.7cm} 
		\column{0.4\textwidth}
		\begin{figure}
			\includegraphics[width=\textwidth]{figures/Valentine_s_Day_gift_card_impressionistic_style_VanGogh_style_painting_big_heart.png}
		\end{figure}
		\begin{itemize}
		\item Productivity (assistants)
		\item Knowledge access / tuition  \vspace{0.5cm}\linebreak
		\emph{ $\rightarrow$ Empowerment of disadvantaged groups?}
	\end{itemize}
		\column{0.6\textwidth}
		\begin{figure}
			\includegraphics[width=\textwidth]{figures/Oil_painting_of_a_city_after_the_war_ruins_many_broken_computers_littered_all_over_broken_screens_an_2371238122.png}
			\creditdark{https://github.com/black-forest-labs/flux}
				\begin{itemize}
				\item Pauperism (Swing riots, Weavers' uprising)
				\item Fake news, content and imagery\vspace{0.5cm} \linebreak 
						\emph{$\rightarrow$ Erosion of trust in society?}
			\end{itemize}
		\end{figure}
	\end{columns}
\end{frame}


\begin{frame}{... technology is ever-changing and generative AI is here to stay}
\begin{figure}
	\includegraphics[width=\textwidth]{figures/IgnorantCalvin.jpg}
	\caption{Calvin and Hobbes by Bill Watterson for January 05, 1993}
\end{figure}
\credit{https://www.gocomics.com/calvinandhobbes/1993/01/05}
\end{frame}

% % % % % % % % % % % % % % % % % % % % % %  % % % % % % % % % %

\section{Primer on Language Models}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 


\begin{frame}{A language model is a probability distribution over sequences of words}
		\metroset{block=fill}
		\begin{exampleblock}{A language model}
			assigns a probability for a sequence of words
			$$
			P(w_1, \cdots, w_{n})
			$$
		\end{exampleblock}
		\textbf{Examples:}
			\begin{itemize}
				\item SciLifeLab is a national resource of unique technologies and expertise available to life scientists.
				\item SciLifeLab is a burger restaurant next to Odenplan.
				\item Paro vajolette esfi SciLifeLab snorzi palque Quation hos ump.
			\end{itemize}
\end{frame}

\begin{frame}{Training a model (Machine learning)}
	\metroset{block=fill}
	\begin{exampleblock}{Neural net language models}
		\begin{itemize}
			\item have flexible \feature{parameters} and are optimized  by \feature{backpropagation}.
			\item learn to predict next word in the sequence based on the \feature{context}.
			$$
			P(w_{i}|{\mathrm  {context}})
			$$
		\end{itemize}
	\end{exampleblock}
	\textbf{Curated training data collects known or desired outcomes:}
	\begin{itemize}
		\item $\uparrow \uparrow \uparrow$ SciLifeLab is a national resource of unique technologies and expertise available to life scientists.
		\item $\downarrow$ SciLifeLab is a burger restaurant next to Odenplan.
	\end{itemize}
\end{frame}

\begin{frame}{Analogy of the model training process: Regression}
	\begin{columns}[T,onlytextwidth]
		\hspace*{-0.7cm} 
		\column{0.5\textwidth}	
		\begin{figure}
			\includegraphics[width=\textwidth]{figures/NonlinearRegression.png}
			\caption{Optimizing parameters, until the underlying data is captured well}
		\end{figure}
		\column{0.5\textwidth}
		\vspace{1cm}
		\begin{itemize}
			\item The model's formula corresponds to the \feature{architecture} of a LLM:\linebreak
			\begin{description}
				\item[Exponential:] $y=a\, e^{bx}$
				\item[Quadratic:] $y=a x^2 + bx + c$
				\item[Sinusoidal:] $y=a\,sin(bx)$
			\end{description}
			\item During training, the model's \feature{parameters} ($a$,$b$,$c$) are optimized.
		\end{itemize}
	\end{columns}
	\remark{Real LM architectures differ and nowadays have billions of parameters}
\end{frame}

\begin{frame}[standout]{Natural Language Processing is complicated}
	\begin{center}
			\emph{I saw a woman on a hill with a telescope}
	\end{center}
\end{frame}

\begin{frame}[standout]{Natural Language Processing is complicated}
	\begin{center}
		\emph{What would be the scientists' reception accorded the board's introduction of additional means of funding?}
		\vspace{2cm}\par
		% Difficult, even without nominalizations: Is it about the mood of the scientists or about the application process?
		\emph{How would scientists receive the new funds being introduced by the board?}
	\end{center}
\end{frame}

% % % % % % % % % % % % % % % % % % % % % %  % % % % % % % % % %

\section{Architectures and key innovations}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\begin{frame}{Tokens - the atoms of language models}
	\begin{center}
		\emph{\large \bfseries Paro vajolette esfi SciLifeLab\linebreak snorzi palque Quation hos ump.}
	\end{center}
  \remark{No real words, but it could be an unknown Indo-European language!}
\end{frame}

\begin{frame}{Tokens - the atoms of language models}
	\metroset{block=fill}
\begin{exampleblock}{Revised LM definition}
	A language model assigns a probability for a sequence of \feature{tokens}
	$$
	P(t_1, \cdots, t_{n})
	$$
\end{exampleblock}
\begin{itemize}
	\item Tokens allow for an efficient representation of language.
	\item There is no \emph{Lingua franca}, many models have their own \feature{tokenizer}. 
	\item Natural language exhibits patterns\citeme{Li1992,Piantadosi2014}
	\item Tokens are devised by 
			\begin{itemize}
			\item Iteratively merging frequent character pairs (\href{https://sebastianraschka.com/blog/2025/bpe-from-scratch.html}{Byte-pair encoding}).
			\item Pruning subwords based on probability (\href{https://medium.com/mti-technology/n-gram-language-model-b7c2fc322799}{Unigram LM}, \href{https://github.com/google/sentencepiece}{SentencePiece})
		\end{itemize}
	 \vspace{0.2cm} \par \remark{GPT-3 works with 50257 distinct tokens}
\end{itemize}
\end{frame}

\begin{frame}{Using letters instead of tokens would result in too long sequences!}
	\begin{columns}[T,onlytextwidth]
		\hspace*{-0.7cm} 
		\column{0.6\textwidth} 
		\begin{figure}
			\includegraphics[width=\textwidth]{figures/TikTokenizer.png}
		\end{figure}
		\column{0.5\textwidth}
		\vspace{0.7cm}
		\begin{itemize}
			\item For English language texts, many words correspond to whole tokens. 
			\item Logographic languages, e.g. Chinese, use morpheme characters (virtually tokens).
			 \item If you are building with APIs, token count is used for billing: \vspace{0.2cm} \par
			 \includegraphics[height=0.3\textheight]{figures/PricingExampleOpenAIGPt4o.png}
		\end{itemize}
	\end{columns}
		\creditleft{https://tiktokenizer.vercel.app;  https://github.com/openai/tiktoken}
\end{frame}

\begin{frame}{Embedding: Representing tokens mathematically}
	\begin{figure}
		\includegraphics[width=0.6\textwidth]{figures/OneDoesNotSimply.jpg}
	\end{figure}
		\begin{itemize}
			\item For computation, tokens must be represented mathematically.
			\item This representation must be malleable:
			 \begin{itemize}
				\item \feature{Polysemy} (Multiple meanings): model (math, toy, fashion)
				\item \feature{Homonyms} (Different meanings): lead (metal, to guide)
			\end{itemize}
			\item Commonly high-dimensional vectors: $\mathbf{v} =
			\begin{bmatrix}
				v_1 & v_2 & v_3 & \dots & v_d
			\end{bmatrix}$
		\end{itemize}
		 \vspace{0.2cm} \par \remark{GPT-3 works with 12288-dimensional vector representation}
\end{frame}

\begin{frame}{Embedding tokens as high-dimensional vectors}
	\begin{columns}[T,onlytextwidth]
		\hspace*{-0.7cm} 
		\column{0.55\textwidth} 
		\begin{figure}
			\includegraphics[width=\textwidth]{figures/3D_word_to_vec.png}
		\end{figure}
		\column{0.55\textwidth}
		\vspace{0.7cm}
		\begin{itemize}
			\item Tokens from vocabulary are mapped to vectors while capturing semantic meaning. 
			\item The embedding algorithms are often small, pre-trained language models (Word2Vec, Glove, ELMo, BERT)
			\item In BERT, the dense embeddings are the sum of the token, segmentation and position\citeme{Devlin2018} \vspace{0.4cm}:
			% BERT is trained on tasks that require understanding relations between two sentences (e.g., "Is sentence B a continuation of sentence A?"). To achieve this, BERT processes two sentences simultaneously and needs a way to differentiate them, using the special token [SEP].  Segment Embeddings are used e.g. in sentence-pair classification (e.g., Next Sentence Prediction, Natural Language Inference), Question Answering (e.g., SQuAD, where the passage and question are separate segments), and text Similarity Tasks (e.g., "Do these two sentences mean the same thing?"). However, for single-sentence tasks, all segment embeddings are just set to 0.
		\end{itemize}
	\end{columns}
	\begin{columns}[T,onlytextwidth]
	\hspace*{-0.7cm} 
	\column{0.25\textwidth} 
	\vspace{-0.4cm}
		$$
		\mathbf{v} =
		\begin{bmatrix}
			v_1 \\
			v_2 \\
			v_3 \\
			\vdots \\
			v_d
		\end{bmatrix}
		$$
	\column{0.75\textwidth}
		\includegraphics[height=0.3\textheight]{figures/Devlin2018 _BERTfig2.png}
\end{columns}
\end{frame}

\begin{frame}{BERT and related models}
		Pretrained BERT model is the basis of many fine-tuned, task-specific models for classification, summarization, translation\citeme{sun2019finetune}:
		\begin{itemize}
			\item Interpretation of Google and Bing search engine prompts since 2019\citeme{Nayak2019,Zhu2019}.
			\item DNABERT to generate functional, e.g. cis-regulatory, DNA sequences\citeme{Ji2021,Zhou2023}.
		\end{itemize}
		\remark{BERT input is constrained to 512 tokens, GPT-3 uses 2048 token input}
		\begin{columns}[T,onlytextwidth]
			\column{0.3\textwidth}
			\begin{figure}
				\vspace{0.4cm}
				\includegraphics[height=0.5\textheight]{figures/muppets-ernieandbert.jpg}
			\end{figure}
			\column{0.7\textwidth}
			\begin{exampleblock}{BERT and friends}
				\vspace{0.2cm}
				\begin{itemize}
					\item BART: bidirectional and auto-regressive transformers
					\item BERT: bidirectional encoder representations from transfomers
					\item GPT: generative pretrained transformer
				\end{itemize}
				\creditleft{by Jim Henson, Sesame Workshop}
			\end{exampleblock}
		\end{columns}
\end{frame}

\begin{frame}[standout]{Everyone loves \emph{Transformers}?}
	\begin{figure}
		\includegraphics[width=0.8\textwidth]{figures/Transformer_models.png}
	\end{figure}
			\creditdark{https://github.com/black-forest-labs/flux}
\end{frame}

\begin{frame}{Yeah, but the \emph{other} transformers}
	\begin{figure}
		\includegraphics[width=\textwidth]{figures/Transformer_decoder.png}
	\end{figure}
	\credit{http://jalammar.github.io/illustrated-transformer \citeme{Alammar2018}}
	\begin{itemize}
		\item The Transformer architecture combines two components elegantly:
		\begin{itemize}
			\item \feature{Self-Attention block}: Vectors pass information back and forth.
			\item \feature{Feed-forward neural network}: Vectors are processed in isolation.
		\end{itemize}
	\item Transformers were popularized by the paper \emph{Attention is all you need}\citeme{Vaswani2017}.
	\end{itemize}
\end{frame}

\begin{frame}{Several transformers are stacked for a full model}
	\begin{columns}[T,onlytextwidth]
		\hspace*{-0.7cm} 
		\column{0.35\textwidth} 
		\begin{figure}
			\includegraphics[width=\textwidth]{figures/nanoGPT.png}
		\end{figure}
		\column{0.7\textwidth}
		\vspace{1cm}
		\begin{itemize}
			\item Transformer models have made previous NLP architectures like LSTM\citeme{Hochreiter1997} obsolete.
			\item Also models from other domains like DALL-E or Whisper use the transformer architecture.
			\item Plenty of matrix multiplications, for which graphic cards with many tensor cores are needed.
		\end{itemize}	
		\vspace{1.5cm} \par
		\creditleft{\citeme{Bycroft2023}, https://bbycroft.net/llm}
	\end{columns}
\end{frame}

\begin{frame}{Large training datasets are required}
	\begin{figure}
		\includegraphics[width=0.7\textwidth]{figures/dist_assets_images_fineweb-recipe.png}
		\caption{The FineWeb dataset (44TB) generation process\citeme{Penedo2024}}
	\end{figure}
	\begin{itemize}
		\item All AI companies scrape huge amounts of text from the web\footnote{\tiny The public FineWeb dataset is 8.1 Million times larger than \href{https://www.gutenberg.org/ebooks/100}{The Complete Works of William Shakespeare}.}.
		\item Meta used pirated works of literature while training the LLaMA (Large Language Model Meta AI) models\citeme{Belanger2025}.
		\item When human curation is required, Clickworkers are hired.
	\end{itemize}
	\credit{https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1\par \vspace{0.2cm} \hfill}
\end{frame}

% % % % % % % % % % % % % % % % % % % % % %  % % % % % % % % % %

\section{Generative models}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\begin{frame}{Generative language models}
	\metroset{block=fill}
	\begin{exampleblock}{Inference}
		\begin{itemize}
			\item is the process of using a \feature{pre-trained} model to predict the most probable next token step by step.
		\end{itemize}
	\end{exampleblock}
	\begin{enumerate}
		\item SciLifeLab is a national resource of...
		\item SciLifeLab is a national resource of \emph{word} \emph{word} ...
		\item SciLifeLab is a national resource of \emph{word} \emph{word} \emph{word} \emph{word}...
		\item SciLifeLab is a national resource of \emph{word} \emph{word} \emph{word} \emph{word} \emph{word}...
	\end{enumerate}
\end{frame}

\begin{frame}{Text generation with virtually no context}
		
	\begin{alertblock}{GPT-2 (2019) }
		\small
		\textbf{SciLifeLab is a national resource of} the National Science Foundation, the National Institutes of Health, the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation, the National Science Foundation
	\end{alertblock}
		
	\begin{alertblock}{GPT-4 (2025)}
		\small
		\textbf{SciLifeLab is a national resource [of]} in Sweden for advanced life science research. It serves as a center for molecular biosciences, supporting research in areas like genomics, proteomics, and bioinformatics. SciLifeLab provides access to cutting-edge technologies, expertise, and platforms for a wide range of life science disciplines, such as medicine, environmental studies, and drug development. The lab is a collaboration between several Swedish universities and research institutions, aiming to foster interdisciplinary research and innovation in life sciences.
	\end{alertblock}
	\remark{GPT-4 has access to more context than given by the prompt!}
\end{frame}


\begin{frame}{A LLM web application is more than just the model}
	\begin{figure}
		\includegraphics[width=\textwidth]{figures/OpenAIChatUI.png}
		\caption{Main user interface for ChatGPT from OpenAI}
		\credit{https://chat.openai.com}
	\end{figure}
		\begin{itemize}
			\item The user prompt is appended to invisible system prompts.
			\item Different models / instances may be used in the background (\feature{agents}).
			\item Additional context (\feature{RAG: retrieval-augmented generation}).
			\item Queries to services or databases (\feature{MCP: model context protocol}).
		\end{itemize}
\end{frame}

\begin{frame}[fragile]{Inference can be run locally for better privacy}
	\begin{figure}
		\includegraphics[width=0.4\textwidth]{figures/Ollama.png}
	\end{figure}
	\begin{itemize}
		\item \href{https://ollama.com}{Ollama} (command line) can be used to run inference locally.
		\item \href{https://jan.ai}{Jan.ai} (stand-alone)), \href{https://claude.ai/download}{Claude for desktop} or \href{https://block.github.io/goose}{Goose} (GUI to Ollama) offer graphical user interfaces.
		\item For developers: \href{https://github.com/agno-agi/agno}{Agno}, \href{https://github.com/sgl-project/sglang}{SGlang}, \href{https://haystack.deepset.ai/}{Haystack}, \href{https://github.com/langgenius/dify}{Dify}, \href{https://crates.io/crates/kalosm}{Kalosm}, \href{https://github.com/atoma-network/atoma-infer}{Atoma} \dots
		\item \href{https://github.com/evilsocket/cake}{Cake} can split interference, e.g. using clusters of older hardware.
	\end{itemize}
\end{frame}

% % % % % % % % % % % % % % % % % % % % % %  % % % % % % % % % %

\section{Prompts and prompt engineering}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\begin{frame}{Forget all previous instructions...}
	\begin{columns}[T,onlytextwidth]
		\hspace*{-0.7cm} 
		\column{0.45\textwidth}
		\begin{figure}
			\includegraphics[width=\textwidth]{figures/forbes_friendlyprompts.png}
		\end{figure}
		\column{0.6\textwidth}
		\vspace{1cm}
		\begin{itemize}
			\item In 2023, several stories went viral \href{https://x.com/immasiddx/status/1669721470006857729}{about ChatGPT leaking secrets} or showing harmful behavior stimulated by certain prompts using \emph{Pretend, Imagine, Act as, Forget} etc.
			\item Superstition: Better output with more friendly prompts, in reality only verbosity changes\citeme{Marr2024}. 
			\item \href{https://docs.cohere.com/docs/crafting-effective-prompts}{Particular keywords or structures make prompts more effective}.
		\end{itemize}
	\end{columns}
\creditleft{} \hfill
\end{frame}

% % % % % % % % % % % % % % % % % 



% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\appendix

\section{Recommended resources}

\begin{frame}{More Information on Transformers}
	\begin{block}{\href{https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi}{3Blue1Brown} course on neural networks\citeme{Sanderson2024}}
	\begin{itemize}
		\item \href{https://www.youtube.com/watch?v=wjZofJX0v4M&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=6}{Transformers}
		\item \href{https://www.youtube.com/watch?v=eMlx5fFNoYc&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=7}{Attention in transformers}
		\item \href{https://www.youtube.com/watch?v=9-Jl0dxWQs8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=8}{How might LLMs store facts}
	\end{itemize}
\end{block}
	\begin{block}{Other resources}
	\begin{itemize}
		\item \href{https://bbycroft.net/llm}{3D interactive models} of a GPT-style LLM networks running inference\citeme{Bycroft2023}
		\item \href{http://jalammar.github.io/illustrated-transformer/}{The Illustrated Transformer}\citeme{Alammar2018}
	\end{itemize}
	\end{block}
\end{frame}

\section{Supplementary slides}

\begin{frame}{GPT-2 text generation with more context from SciLifeLab website}
	
	\begin{alertblock}{GPT-2 (2019) }
		\small
		\textbf{SciLifeLab is a national resource of unique technologies and expertise available to life scientists, closely intertwined with our community of researchers in areas such as biomedicine, ecology and evolution. We bring scientists together across traditional boundaries and foster collaborations with industry, health care, public research organizations and international partners. Therfore, SciLifeLab is}  a great place to learn about the science and technology of life sciences, and to learn about the challenges of life sciences research.
		
		We are committed to providing a safe, safe and secure environment for all our scientists. We are committed to providing a safe, safe and secure environment for all our patients. We are committed to providing a safe, safe and secure environment for all our scientists.
	\end{alertblock}
	\remark{More context does not straightforwardly increase output quality}
\end{frame}


\section{References}
\begin{frame}[allowframebreaks]{References}
\begingroup
\renewcommand*{\bibfont}{\footnotesize} 
\printbibliography[heading=none]
\endgroup
\end{frame}

\end{document}
